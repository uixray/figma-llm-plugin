import { ProviderType, GenerationSettings, LMStudioConfig, YandexConfig, OpenAICompatibleConfig } from '../shared/types';
import { StorageManager } from './storage-manager';
import { estimateTokens } from '../shared/utils';
import { SimpleAbortSignal } from '../shared/abort-helper';
import { PROVIDER_CONFIGS } from '../shared/providers';
import { getAllProviderConfigs } from '../shared/provider-converter';

/**
 * –ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ (V2)
 */
export interface GenerationRequest {
  providerId: string; // V2: ID –∏–∑ UserProviderConfig
  prompt: string;
  systemPrompt?: string;
  /** –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ messages –¥–ª—è few-shot (user/assistant –ø–∞—Ä—ã) */
  fewShotMessages?: Array<{ role: 'user' | 'assistant'; text: string }>;
  settings: GenerationSettings;
  signal: SimpleAbortSignal;
  onChunk: (chunk: string, estimatedTokens: number) => void;
}

/**
 * –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM API
 */
export class ApiClient {
  private storageManager: StorageManager;

  constructor(storageManager: StorageManager) {
    this.storageManager = storageManager;
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (V2 - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç providerId)
   */
  async generateText(request: GenerationRequest): Promise<void> {
    console.log('[ApiClient] generateText called for providerId:', request.providerId);

    const settings = await this.storageManager.loadSettings();
    console.log('[ApiClient] Settings loaded');

    // V2.1: –û–±—ä–µ–¥–∏–Ω—è–µ–º Legacy –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ Provider Groups
    const legacyConfigs = settings.providerConfigs || [];
    const groups = settings.providerGroups || [];
    const allConfigs = getAllProviderConfigs(legacyConfigs, groups);
    console.log('[ApiClient] Total available configs (Legacy + Groups):', allConfigs.length);

    // –ò—â–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ ID
    const userConfig = allConfigs.find(c => c.id === request.providerId);
    console.log('[ApiClient] User config:', JSON.stringify(userConfig, null, 2));

    if (!userConfig || !userConfig.enabled) {
      console.error('[ApiClient] Provider not configured or disabled');
      throw new Error(`Provider ${request.providerId} is not configured or disabled`);
    }

    // V2: –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    const baseConfigId = userConfig.baseConfigId;
    console.log('[ApiClient] Using base config:', baseConfigId);

    // –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ñ–∏–≥ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –≤ PROVIDER_CONFIGS
    let providerConfig = PROVIDER_CONFIGS.find(p => p.id === baseConfigId);

    // Handle custom "Other" provider (not in PROVIDER_CONFIGS)
    if (!providerConfig && baseConfigId === 'other-custom') {
      providerConfig = {
        id: 'other-custom',
        name: userConfig.modelName || 'Custom Model',
        provider: 'other',
        description: 'User-defined custom provider',
        model: userConfig.modelName || 'custom-model',
        apiUrl: userConfig.customUrl || '',
        requiresProxy: false,
        pricing: { input: 0, output: 0 },
      };
    }

    if (!providerConfig) {
      console.error('[ApiClient] Provider config not found:', baseConfigId);
      throw new Error(`Provider config not found: ${baseConfigId}`);
    }

    console.log('[ApiClient] Provider config found:', providerConfig.name);

    // V2: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (TODO: –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –Ω–∞ Provider –∫–ª–∞—Å—Å—ã)
    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø–æ provider field
    if (providerConfig.provider === 'lmstudio') {
      // LM Studio –¢–†–ï–ë–£–ï–¢ customUrl (–∞–¥—Ä–µ—Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞)
      if (!userConfig.customUrl) {
        throw new Error(
          'üñ•Ô∏è LM Studio requires a server URL.\n' +
          'Go to Settings ‚Üí edit your LM Studio group ‚Üí enter your local server address.\n' +
          'Default: http://127.0.0.1:1234\n' +
          'Make sure LM Studio is running and the server is started.'
        );
      }

      console.log('[ApiClient] LM Studio URL:', userConfig.customUrl);

      // Append /v1 if not already present (LM Studio API is always at /v1)
      const lmBaseUrl = (() => {
        const base = (userConfig.customUrl || '').replace(/\/+$/, '');
        return base.endsWith('/v1') ? base : `${base}/v1`;
      })();

      const legacyConfig: LMStudioConfig = {
        enabled: userConfig.enabled,
        apiKey: userConfig.apiKey || 'not-required', // LM Studio –Ω–µ —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á
        baseUrl: lmBaseUrl,
      };
      return this.generateWithLMStudio(request, legacyConfig);
    } else if (providerConfig.provider === 'yandex') {
      // –î–ª—è Yandex –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ folderId
      if (!userConfig.folderId || userConfig.folderId.includes('YOUR_FOLDER_ID')) {
        throw new Error(
          'Yandex provider requires Folder ID. Please edit the provider in Settings and specify your Yandex Cloud Folder ID (found at cloud.yandex.ru/console)'
        );
      }

      // –°—Ç—Ä–æ–∏–º modelUri –∏–∑ folderId –∏ model
      // –§–æ—Ä–º–∞—Ç: gpt://<folderId>/<model>
      const modelUri = `gpt://${userConfig.folderId}/${providerConfig.model}`;
      console.log('[ApiClient] Yandex modelUri:', modelUri);

      const legacyConfig: YandexConfig = {
        enabled: userConfig.enabled,
        apiKey: userConfig.apiKey,
        folderId: userConfig.folderId,
        model: modelUri,
      };
      return this.generateWithYandex(request, legacyConfig);
    } else if (['openai', 'claude', 'gemini', 'mistral', 'groq', 'cohere'].includes(providerConfig.provider)) {
      // Resolve API URL: user custom URL > global proxy override > default provider URL
      let resolvedApiUrl = userConfig.customUrl || providerConfig.apiUrl;

      // Apply global proxy setting if available
      if (!userConfig.customUrl && settings.globalProxyUrl) {
        if (settings.globalProxyUrl === 'none') {
          // Direct connection ‚Äî strip proxy prefix, use provider's native API
          // Only if the default URL is a proxy URL
          resolvedApiUrl = providerConfig.apiUrl;
        } else {
          // Custom proxy ‚Äî replace proxy.uixray.tech with user's proxy
          resolvedApiUrl = providerConfig.apiUrl.replace(
            'https://proxy.uixray.tech',
            settings.globalProxyUrl.replace(/\/$/, '')
          );
        }
      }

      const legacyConfig: OpenAICompatibleConfig = {
        enabled: userConfig.enabled,
        apiKey: userConfig.apiKey,
        apiUrl: resolvedApiUrl,
        model: providerConfig.model,
      };
      return this.generateWithOpenAI(request, legacyConfig);
    } else if (providerConfig.provider === 'other' || baseConfigId === 'other-custom') {
      // Custom "Other" provider ‚Äî uses OpenAI-compatible API
      const customApiUrl = userConfig.customUrl;
      const customModel = userConfig.modelName || 'custom-model';

      if (!customApiUrl) {
        throw new Error(
          'Custom provider requires API Base URL. Please edit the provider group in Settings and specify the API endpoint.'
        );
      }

      console.log('[ApiClient] Custom provider:', customApiUrl, 'model:', customModel);

      const legacyConfig: OpenAICompatibleConfig = {
        enabled: userConfig.enabled,
        apiKey: userConfig.apiKey,
        apiUrl: customApiUrl,
        model: customModel,
      };
      return this.generateWithOpenAI(request, legacyConfig);
    }

    console.error('[ApiClient] Unknown provider type:', providerConfig.provider);
    throw new Error(`Unknown provider type: ${providerConfig.provider}`);
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ LM Studio
   */
  private async generateWithLMStudio(
    request: GenerationRequest,
    config: LMStudioConfig
  ): Promise<void> {
    console.log('[ApiClient] ===== generateWithLMStudio START =====');
    console.log('[ApiClient] config:', JSON.stringify(config, null, 2));
    console.log('[ApiClient] request.settings:', JSON.stringify(request.settings, null, 2));

    const url = config.useProxy && config.proxyUrl
      ? `${config.proxyUrl}/v1/chat/completions`
      : `${config.baseUrl}/chat/completions`;

    console.log('[ApiClient] Computed URL:', url);

    // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming (response.body undefined)
    // –í—Å–µ–≥–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º stream: false
    // –§–æ—Ä–º–∏—Ä—É–µ–º messages —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π few-shot
    const lmsMessages: Array<{ role: string; content: string }> = [];
    if (request.systemPrompt) {
      lmsMessages.push({ role: 'system', content: request.systemPrompt });
    }
    if (request.fewShotMessages) {
      for (const msg of request.fewShotMessages) {
        lmsMessages.push({ role: msg.role, content: msg.text });
      }
    }
    lmsMessages.push({ role: 'user', content: request.prompt });

    const body = {
      model: config.model,
      messages: lmsMessages,
      temperature: request.settings.temperature,
      max_tokens: request.settings.maxTokens,
      stream: false, // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º streaming
    };

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
    request.signal.throwIfAborted();

    console.log('[ApiClient] LM Studio request URL:', url);
    console.log('[ApiClient] LM Studio request body:', JSON.stringify(body, null, 2));

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });

    console.log('[ApiClient] LM Studio response status:', response.status);
    console.log('[ApiClient] LM Studio response ok:', response.ok);
    console.log('[ApiClient] LM Studio response body exists:', !!response.body);

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
    request.signal.throwIfAborted();

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[ApiClient] LM Studio error response:', errorText);
      throw new Error(`LM Studio API error: ${response.status} ${response.statusText}: ${errorText}`);
    }

    // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç response.body (ReadableStream)
    // –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º non-streaming —Ä–µ–∂–∏–º –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
    console.warn('[ApiClient] Figma fetch does not support streaming. Using non-streaming mode.');
    await this.handleNonStreamingResponse(response, request.onChunk);
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Yandex Cloud
   */
  private async generateWithYandex(
    request: GenerationRequest,
    config: YandexConfig
  ): Promise<void> {
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ CORS –≤ Figma –ø–ª–∞–≥–∏–Ω–∞—Ö
    const url = 'https://proxy.uixray.tech/api/yandex';

    // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming (response.body undefined)
    // config.model —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π modelUri: gpt://<folderId>/<model>
    // –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –µ–≥–æ –≤ customUrl –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    // –§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ messages
    const messages: Array<{ role: string; text: string }> = [];

    // 1. System prompt
    if (request.systemPrompt) {
      messages.push({ role: 'system', text: request.systemPrompt });
    }

    // 2. Few-shot examples (user ‚Üí assistant –ø–∞—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç—É)
    if (request.fewShotMessages && request.fewShotMessages.length > 0) {
      for (const msg of request.fewShotMessages) {
        messages.push({ role: msg.role, text: msg.text });
      }
    }

    // 3. –§–∏–Ω–∞–ª—å–Ω—ã–π user prompt
    messages.push({ role: 'user', text: request.prompt });

    const body = {
      modelUri: config.model, // –ü–æ–ª–Ω—ã–π URI, –Ω–∞–ø—Ä–∏–º–µ—Ä gpt://b1g.../yandexgpt-lite/latest
      completionOptions: {
        stream: false, // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º streaming
        temperature: request.settings.temperature,
        maxTokens: String(request.settings.maxTokens),
        // –û—Ç–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ "–¥—É–º–∞–ª–∞ –≤—Å–ª—É—Ö"
        reasoningOptions: {
          mode: 'DISABLED',
        },
      },
      messages,
    };

    console.log('[ApiClient] Yandex request URL:', url);
    console.log('[ApiClient] Yandex request body:', JSON.stringify(body, null, 2));

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
    request.signal.throwIfAborted();

    console.log('[ApiClient] Sending request to Yandex...');

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Api-Key ${config.apiKey}`,
        },
        body: JSON.stringify(body),
      });

      console.log('[ApiClient] Yandex response status:', response.status);

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
      request.signal.throwIfAborted();

      if (!response.ok) {
        const errorText = await response.text();
        console.error('[ApiClient] Yandex error response:', errorText);
        throw new Error(`Yandex API error: ${response.status} ${errorText}`);
      }

      // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç response.body (ReadableStream)
      // –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º non-streaming —Ä–µ–∂–∏–º –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
      console.warn('[ApiClient] Figma fetch does not support streaming. Using non-streaming mode.');
      await this.handleYandexNonStreamingResponse(response, request.onChunk);
    } catch (error) {
      console.error('[ApiClient] Yandex request failed:', error);
      throw error;
    }
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ-streaming –æ—Ç–≤–µ—Ç–∞ –æ—Ç Yandex Cloud
   * –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç OpenAI
   * NOTE: Figma plugin sandbox fetch() may not expose response.headers.
   */
  private async handleYandexNonStreamingResponse(
    response: Response,
    onChunk: (chunk: string, tokens: number) => void
  ): Promise<void> {
    if (response.headers && typeof response.headers.get === 'function') {
      const contentType = response.headers.get('Content-Type') || '';
      if (!contentType.includes('application/json')) {
        const text = await response.text();
        console.error('[ApiClient] Yandex unexpected Content-Type:', contentType);
        console.error('[ApiClient] Response text (first 200 chars):', text.substring(0, 200));
        throw new Error(`Expected JSON response but received ${contentType}. Check Yandex API key and folder ID.`);
      }
    }

    let data;
    try {
      data = await response.json();
    } catch (e: any) {
      console.error('[ApiClient] Yandex JSON parse error:', e);
      throw new Error(`Failed to parse Yandex JSON response: ${e.message}`);
    }

    console.log('[ApiClient] Yandex response data:', JSON.stringify(data, null, 2));

    // Yandex —Ñ–æ—Ä–º–∞—Ç: { result: { alternatives: [{ message: { text: "..." } }] } }
    const content = data.result?.alternatives?.[0]?.message?.text;

    if (!content) {
      console.error('[ApiClient] Invalid Yandex response format:', data);
      throw new Error('Empty response from Yandex API');
    }

    const estimatedTokens = estimateTokens(content);
    onChunk(content, estimatedTokens);
  }

  /**
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI-compatible API
   */
  private async generateWithOpenAI(
    request: GenerationRequest,
    config: OpenAICompatibleConfig
  ): Promise<void> {
    const url = `${config.baseUrl}/chat/completions`;

    // –§–æ—Ä–º–∏—Ä—É–µ–º messages —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π few-shot
    const oaiMessages: Array<{ role: string; content: string }> = [];
    if (request.systemPrompt) {
      oaiMessages.push({ role: 'system', content: request.systemPrompt });
    }
    if (request.fewShotMessages) {
      for (const msg of request.fewShotMessages) {
        oaiMessages.push({ role: msg.role, content: msg.text });
      }
    }
    oaiMessages.push({ role: 'user', content: request.prompt });

    // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming (response.body undefined)
    const body = {
      model: config.model,
      messages: oaiMessages,
      temperature: request.settings.temperature,
      max_tokens: request.settings.maxTokens,
      stream: false, // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º streaming
    };

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
    request.signal.throwIfAborted();

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`,
        ...(config.organization ? { 'OpenAI-Organization': config.organization } : {}),
      },
      body: JSON.stringify(body),
    });

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º abort –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
    request.signal.throwIfAborted();

    if (!response.ok) {
      const contentType = response.headers.get('Content-Type') || '';
      let errorMessage = `OpenAI API error: ${response.status} ${response.statusText}`;

      try {
        const errorText = await response.text();
        console.error('[ApiClient] OpenAI error response:', errorText);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç JSON
        if (contentType.includes('application/json')) {
          try {
            const errorJson = JSON.parse(errorText);
            errorMessage += `: ${errorJson.error?.message || errorText}`;
          } catch {
            errorMessage += `: ${errorText}`;
          }
        } else {
          // HTML –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
          errorMessage += ' (received non-JSON response - check API URL and key)';
        }
      } catch {
        // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —á—Ç–µ–Ω–∏—è body
      }

      throw new Error(errorMessage);
    }

    // –í–ê–ñ–ù–û: Figma's fetch –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç response.body (ReadableStream)
    // –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º non-streaming —Ä–µ–∂–∏–º –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
    console.warn('[ApiClient] Figma fetch does not support streaming. Using non-streaming mode.');
    await this.handleNonStreamingResponse(response, request.onChunk);
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∫–∞ streaming –æ—Ç–≤–µ—Ç–∞
   */
  private async handleStreamingResponse(
    response: Response,
    onChunk: (chunk: string, tokens: number) => void
  ): Promise<void> {
    console.log('[ApiClient] handleStreamingResponse called');
    console.log('[ApiClient] response.body:', response.body);
    console.log('[ApiClient] response.bodyUsed:', response.bodyUsed);

    const reader = response.body?.getReader();
    if (!reader) {
      console.error('[ApiClient] Response body is null or undefined');
      throw new Error('Response body is not readable');
    }

    console.log('[ApiClient] Reader created successfully');

    const decoder = new TextDecoder();
    let buffer = '';

    try {
      while (true) {
        const { done, value } = await reader.read();

        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);

            if (data === '[DONE]') {
              return;
            }

            try {
              const parsed = JSON.parse(data);
              const chunk = parsed.choices?.[0]?.delta?.content;

              if (chunk) {
                const estimatedTokens = estimateTokens(chunk);
                onChunk(chunk, estimatedTokens);
              }
            } catch (e) {
              console.warn('Failed to parse SSE chunk:', e);
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ-streaming –æ—Ç–≤–µ—Ç–∞
   * NOTE: Figma plugin sandbox fetch() returns a stripped Response ‚Äî response.headers
   * and response.body may be undefined. Skip Content-Type check and go straight to json().
   */
  private async handleNonStreamingResponse(
    response: Response,
    onChunk: (chunk: string, tokens: number) => void
  ): Promise<void> {
    // Try Content-Type check only when headers are available (not in Figma sandbox)
    if (response.headers && typeof response.headers.get === 'function') {
      const contentType = response.headers.get('Content-Type') || '';
      if (!contentType.includes('application/json')) {
        const text = await response.text();
        console.error('[ApiClient] Unexpected Content-Type:', contentType);
        console.error('[ApiClient] Response text (first 200 chars):', text.substring(0, 200));
        throw new Error(`Expected JSON response but received ${contentType}. Check API URL and key. Response starts with: ${text.substring(0, 100)}`);
      }
    }

    let data;
    try {
      data = await response.json();
    } catch (e: any) {
      console.error('[ApiClient] JSON parse error:', e);
      throw new Error(`Failed to parse JSON response: ${e.message}. Check that the provider URL is correct and returns JSON.`);
    }

    const content = data.choices?.[0]?.message?.content;

    if (!content) {
      console.error('[ApiClient] Empty content in response:', JSON.stringify(data, null, 2));
      throw new Error('Empty response from API');
    }

    const estimatedTokens = estimateTokens(content);
    onChunk(content, estimatedTokens);
  }
}
